{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='your module name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rumor0!:脑出血不能引起病人剧烈的头痛。\n",
      "不能    1\n",
      "剧烈    1\n",
      "头痛    1\n",
      "引起    1\n",
      "病人    1\n",
      "Name: rumor0!, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv',index_col=0)\n",
    "index = df.index\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=df.seg_text)\n",
    "                        .toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "                                     counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize,)\n",
    "tfidf_docs =tfidf.fit_transform(raw_documents=df.text).toarray()\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('rumor0:'+df.loc['rumor0'].text)\n",
    "    print(bow_docs.loc['rumor0'][bow_docs.loc['rumor0'] > 0].head())\n",
    "except KeyError:\n",
    "    print('rumor0!:'+df.loc['rumor0!'].text)\n",
    "    print(bow_docs.loc['rumor0!'][bow_docs.loc['rumor0!'] > 0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.sentiments\n",
    "sentiment_dict = np.load('sentiment_dict.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia100 = LDiA(n_components=100, learning_method='batch')\n",
    "ldia100 = ldia100.fit(bow_docs)\n",
    "ldia_topic_vectors_100 = ldia100.transform(bow_docs)\n",
    "columns100 = ['topic{}'.format(i) for i in range(ldia100.components_.shape[0])]\n",
    "ldia_topic_vectors_100_doc = pd.DataFrame(ldia_topic_vectors_100,index=index, columns=columns100)\n",
    "ldia_topic_vectors_100_sentiment = np.array([ldia_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia_topic_vectors_100_sentiment_doc = pd.DataFrame(ldia_topic_vectors_100_sentiment,index=index, columns=columns100)\n",
    "\n",
    "ldia300 = LDiA(n_components=300, learning_method='batch')\n",
    "ldia300 = ldia100.fit(bow_docs)\n",
    "ldia_topic_vectors_300 = ldia300.transform(bow_docs)\n",
    "columns300 = ['topic{}'.format(i) for i in range(ldia300.components_.shape[0])]\n",
    "ldia_topic_vectors_300_doc = pd.DataFrame(ldia_topic_vectors_300,index=index, columns=columns300)\n",
    "ldia_topic_vectors_300_sentiment = np.array([ldia_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia_topic_vectors_300_sentiment_doc = pd.DataFrame(ldia_topic_vectors_300_sentiment,index=index, columns=columns300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "k=min(len(tfidf.vocabulary_),len(df))\n",
    "k=100\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors_100 = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors_100_doc = pd.DataFrame(pca_topic_vectors_100, columns=columns, index=index)\n",
    "pca_topic_vectors_100_sentiment = np.array([pca_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_topic_vectors_100_sentiment_doc = pd.DataFrame(pca_topic_vectors_100_sentiment, columns=columns, index=index)\n",
    "k=300\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors_300 = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors_300_doc = pd.DataFrame(pca_topic_vectors_300, columns=columns, index=index)\n",
    "pca_topic_vectors_300_sentiment = np.array([pca_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_topic_vectors_300_sentiment_doc = pd.DataFrame(pca_topic_vectors_300_sentiment, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_\n",
    "#根据词项的频率对词汇表进行排序\n",
    "#当对某个不按照最左边元素排序的序列解压并在排序后重新压缩时，可以使用zip(*sorted(zip(...)))\n",
    "column_nums , terms = zip(*sorted(zip(tfidf.vocabulary_.values(), tfidf.vocabulary_.keys())))\n",
    "weights = pd.DataFrame(pca.components_, columns=terms,\n",
    "                       index = ['topic{}'.format(i) for i in range(pca.components_.shape[0])])\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100,n_iter=100)\n",
    "svd_topic_vectors_100 = svd.fit_transform(tfidf_docs)\n",
    "svd_topic_vectors_100 = (svd_topic_vectors_100.T / np.linalg.norm(svd_topic_vectors_100,axis=1)).T\n",
    "svd_topic_vectors_100_doc = pd.DataFrame(svd_topic_vectors_100, columns=columns[:100], index=index)\n",
    "svd_topic_vectors_100_sentiment = np.array([svd_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "svd_topic_vectors_100_sentiment_doc = pd.DataFrame(svd_topic_vectors_100_sentiment, columns=columns[:100], index=index)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300,n_iter=100)\n",
    "svd_topic_vectors_300 = svd.fit_transform(tfidf_docs)\n",
    "svd_topic_vectors_300 = (svd_topic_vectors_300.T / np.linalg.norm(svd_topic_vectors_300,axis=1)).T\n",
    "svd_topic_vectors_300_doc = pd.DataFrame(svd_topic_vectors_300, columns=columns[:300], index=index)\n",
    "svd_topic_vectors_300_sentiment = np.array([svd_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "svd_topic_vectors_300_sentiment_doc = pd.DataFrame(svd_topic_vectors_300_sentiment, columns=columns[:300], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def lda_predic (name,vector,df,over_sampling=False):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(vector,df.rumor,test_size=0.2 ,random_state=42)\n",
    "    if over_sampling:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        # 进行过采样\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        print(\"过采样前训练集中类别0的数量：\", sum(y_train==0))\n",
    "        print(\"过采样前训练集中类别1的数量：\", sum(y_train==1))\n",
    "\n",
    "        print(\"过采样后训练集中类别0的数量：\", sum(y_train_res==0))\n",
    "        print(\"过采样后训练集中类别1的数量：\", sum(y_train_res==1))\n",
    "\n",
    "    lda = LDA(n_components=1)\n",
    "    lda = lda.fit(X_train_res, y_train_res)\n",
    "    df[name] = lda.predict(vector)\n",
    "    round(float(lda.score(X_test, y_test)), 3)\n",
    "    from sklearn import metrics\n",
    "    y_true = df.loc[y_test.index].rumor\n",
    "    y_pred = df.loc[y_test.index][name]\n",
    "    loss_total = 0\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=[\"non-rumor\",\"rumor\"], digits=4)\n",
    "    confusion = metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(report)\n",
    "    with open(\"report/\"+name+'.txt', 'w', encoding='utf-8') as f:\n",
    "        print(report,file=f)\n",
    "    print(confusion)\n",
    "    with open(\"report_matrix/\"+name+'_matrix'+'.txt', 'w', encoding='utf-8') as f:\n",
    "        print(confusion,file=f)\n",
    "    np.savez(\"report_matrix/\"+name+'_matrix'+'.npz', matrix= confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9388    0.4986    0.6513       369\n",
      "       rumor     0.4875    0.9362    0.6412       188\n",
      "\n",
      "    accuracy                         0.6463       557\n",
      "   macro avg     0.7132    0.7174    0.6462       557\n",
      "weighted avg     0.7865    0.6463    0.6479       557\n",
      "\n",
      "[[184 185]\n",
      " [ 12 176]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('tfidf',tfidf_docs,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9522    0.7019    0.8081       369\n",
      "       rumor     0.6140    0.9309    0.7400       188\n",
      "\n",
      "    accuracy                         0.7792       557\n",
      "   macro avg     0.7831    0.8164    0.7740       557\n",
      "weighted avg     0.8381    0.7792    0.7851       557\n",
      "\n",
      "[[259 110]\n",
      " [ 13 175]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_100',pca_topic_vectors_100_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9483    0.7453    0.8346       369\n",
      "       rumor     0.6479    0.9202    0.7604       188\n",
      "\n",
      "    accuracy                         0.8043       557\n",
      "   macro avg     0.7981    0.8327    0.7975       557\n",
      "weighted avg     0.8469    0.8043    0.8096       557\n",
      "\n",
      "[[275  94]\n",
      " [ 15 173]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_300',pca_topic_vectors_300_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9027    0.7290    0.8066       369\n",
      "       rumor     0.6139    0.8457    0.7114       188\n",
      "\n",
      "    accuracy                         0.7684       557\n",
      "   macro avg     0.7583    0.7874    0.7590       557\n",
      "weighted avg     0.8052    0.7684    0.7745       557\n",
      "\n",
      "[[269 100]\n",
      " [ 29 159]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_100_sentiment',pca_topic_vectors_100_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9133    0.7425    0.8191       369\n",
      "       rumor     0.6304    0.8617    0.7281       188\n",
      "\n",
      "    accuracy                         0.7828       557\n",
      "   macro avg     0.7718    0.8021    0.7736       557\n",
      "weighted avg     0.8178    0.7828    0.7884       557\n",
      "\n",
      "[[274  95]\n",
      " [ 26 162]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_300_sentiment',pca_topic_vectors_300_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8635    0.6856    0.7644       369\n",
      "       rumor     0.5606    0.7872    0.6549       188\n",
      "\n",
      "    accuracy                         0.7199       557\n",
      "   macro avg     0.7120    0.7364    0.7096       557\n",
      "weighted avg     0.7613    0.7199    0.7274       557\n",
      "\n",
      "[[253 116]\n",
      " [ 40 148]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_100',svd_topic_vectors_100_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9357    0.7100    0.8074       369\n",
      "       rumor     0.6137    0.9043    0.7312       188\n",
      "\n",
      "    accuracy                         0.7756       557\n",
      "   macro avg     0.7747    0.8071    0.7693       557\n",
      "weighted avg     0.8270    0.7756    0.7817       557\n",
      "\n",
      "[[262 107]\n",
      " [ 18 170]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_300',svd_topic_vectors_300_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8653    0.6965    0.7718       369\n",
      "       rumor     0.5692    0.7872    0.6607       188\n",
      "\n",
      "    accuracy                         0.7271       557\n",
      "   macro avg     0.7173    0.7419    0.7162       557\n",
      "weighted avg     0.7654    0.7271    0.7343       557\n",
      "\n",
      "[[257 112]\n",
      " [ 40 148]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_100_sentiment',svd_topic_vectors_100_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9352    0.6260    0.7500       369\n",
      "       rumor     0.5548    0.9149    0.6908       188\n",
      "\n",
      "    accuracy                         0.7235       557\n",
      "   macro avg     0.7450    0.7705    0.7204       557\n",
      "weighted avg     0.8068    0.7235    0.7300       557\n",
      "\n",
      "[[231 138]\n",
      " [ 16 172]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_300_sentiment',svd_topic_vectors_300_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7483    0.5962    0.6637       369\n",
      "       rumor     0.4335    0.6064    0.5055       188\n",
      "\n",
      "    accuracy                         0.5996       557\n",
      "   macro avg     0.5909    0.6013    0.5846       557\n",
      "weighted avg     0.6420    0.5996    0.6103       557\n",
      "\n",
      "[[220 149]\n",
      " [ 74 114]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_100',ldia_topic_vectors_100_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7509    0.5881    0.6596       369\n",
      "       rumor     0.4328    0.6170    0.5088       188\n",
      "\n",
      "    accuracy                         0.5978       557\n",
      "   macro avg     0.5919    0.6025    0.5842       557\n",
      "weighted avg     0.6435    0.5978    0.6087       557\n",
      "\n",
      "[[217 152]\n",
      " [ 72 116]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_300',ldia_topic_vectors_300_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7754    0.5989    0.6758       369\n",
      "       rumor     0.4559    0.6596    0.5391       188\n",
      "\n",
      "    accuracy                         0.6194       557\n",
      "   macro avg     0.6157    0.6292    0.6075       557\n",
      "weighted avg     0.6676    0.6194    0.6297       557\n",
      "\n",
      "[[221 148]\n",
      " [ 64 124]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_100_sentiment',ldia_topic_vectors_100_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7345    0.5474    0.6273       369\n",
      "       rumor     0.4078    0.6117    0.4894       188\n",
      "\n",
      "    accuracy                         0.5691       557\n",
      "   macro avg     0.5712    0.5796    0.5583       557\n",
      "weighted avg     0.6243    0.5691    0.5808       557\n",
      "\n",
      "[[202 167]\n",
      " [ 73 115]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_300_sentiment',ldia_topic_vectors_300_sentiment_doc,df,over_sampling=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d40a5097db4c11f3975da84c5456511726d6f156896da7e31bb0c17b283947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

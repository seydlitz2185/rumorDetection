{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='your module name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rumor0!:脑出血不能引起病人剧烈的头痛。\n",
      "不能    1\n",
      "剧烈    1\n",
      "头痛    1\n",
      "引起    1\n",
      "病人    1\n",
      "Name: rumor0!, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv',index_col=0)\n",
    "index = df.index\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=df.seg_text)\n",
    "                        .toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "                                     counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize,)\n",
    "tfidf_docs =tfidf.fit_transform(raw_documents=df.text).toarray()\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('rumor0:'+df.loc['rumor0'].text)\n",
    "    print(bow_docs.loc['rumor0'][bow_docs.loc['rumor0'] > 0].head())\n",
    "except KeyError:\n",
    "    print('rumor0!:'+df.loc['rumor0!'].text)\n",
    "    print(bow_docs.loc['rumor0!'][bow_docs.loc['rumor0!'] > 0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.sentiments\n",
    "sentiment_dict = np.load('sentiment_dict.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ldia100_topic_vectors_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/cuda/rumorDetection/pygp_topic_analysis_all_sentiment.ipynb 单元格 4\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f697270726f6a227d/root/cuda/rumorDetection/pygp_topic_analysis_all_sentiment.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ldia_topic_vectors_100_doc \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(ldia_topic_vectors_100,index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns100)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f697270726f6a227d/root/cuda/rumorDetection/pygp_topic_analysis_all_sentiment.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m ldia_topic_vectors_100_sentiment \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([ldia_topic_vectors_100[i]\u001b[39m*\u001b[39msentiments[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sentiments))])\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f697270726f6a227d/root/cuda/rumorDetection/pygp_topic_analysis_all_sentiment.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m ldia_topic_vectors_100_sentiment_doc \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(ldia100_topic_vectors_sentiment,index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns100)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f697270726f6a227d/root/cuda/rumorDetection/pygp_topic_analysis_all_sentiment.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m ldia300 \u001b[39m=\u001b[39m LDiA(n_components\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m, learning_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f697270726f6a227d/root/cuda/rumorDetection/pygp_topic_analysis_all_sentiment.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m ldia300 \u001b[39m=\u001b[39m ldia100\u001b[39m.\u001b[39mfit(bow_docs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ldia100_topic_vectors_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia100 = LDiA(n_components=100, learning_method='batch')\n",
    "ldia100 = ldia100.fit(bow_docs)\n",
    "ldia_topic_vectors_100 = ldia100.transform(bow_docs)\n",
    "columns100 = ['topic{}'.format(i) for i in range(ldia100.components_.shape[0])]\n",
    "ldia_topic_vectors_100_doc = pd.DataFrame(ldia_topic_vectors_100,index=index, columns=columns100)\n",
    "ldia_topic_vectors_100_sentiment = np.array([ldia_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia_topic_vectors_100_sentiment_doc = pd.DataFrame(ldia_topic_vectors_100_sentiment,index=index, columns=columns100)\n",
    "\n",
    "ldia300 = LDiA(n_components=300, learning_method='batch')\n",
    "ldia300 = ldia100.fit(bow_docs)\n",
    "ldia_topic_vectors_300 = ldia300.transform(bow_docs)\n",
    "columns300 = ['topic{}'.format(i) for i in range(ldia300.components_.shape[0])]\n",
    "ldia_topic_vectors_300_doc = pd.DataFrame(ldia_topic_vectors_300,index=index, columns=columns300)\n",
    "ldia_topic_vectors_300_sentiment = np.array([ldia_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia_topic_vectors_300_sentiment_doc = pd.DataFrame(ldia_topic_vectors_300_sentiment,index=index, columns=columns300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "k=min(len(tfidf.vocabulary_),len(df))\n",
    "k=100\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors_100 = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors_100_doc = pd.DataFrame(pca_topic_vectors_100, columns=columns, index=index)\n",
    "pca_topic_vectors_100_sentiment = np.array([pca_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_topic_vectors_100_sentiment_doc = pd.DataFrame(pca_topic_vectors_100_sentiment, columns=columns, index=index)\n",
    "k=300\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors_300 = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors_300_doc = pd.DataFrame(pca_topic_vectors_300, columns=columns, index=index)\n",
    "pca_topic_vectors_300_sentiment = np.array([pca_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_topic_vectors_300_sentiment_doc = pd.DataFrame(pca_topic_vectors_300_sentiment, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_\n",
    "#根据词项的频率对词汇表进行排序\n",
    "#当对某个不按照最左边元素排序的序列解压并在排序后重新压缩时，可以使用zip(*sorted(zip(...)))\n",
    "column_nums , terms = zip(*sorted(zip(tfidf.vocabulary_.values(), tfidf.vocabulary_.keys())))\n",
    "weights = pd.DataFrame(pca.components_, columns=terms,\n",
    "                       index = ['topic{}'.format(i) for i in range(pca.components_.shape[0])])\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100,n_iter=100)\n",
    "svd_topic_vectors_100 = svd.fit_transform(tfidf_docs)\n",
    "svd_topic_vectors_100 = (svd_topic_vectors_100.T / np.linalg.norm(svd_topic_vectors_100,axis=1)).T\n",
    "svd_topic_vectors_100_doc = pd.DataFrame(svd_topic_vectors_100, columns=columns[:100], index=index)\n",
    "svd_topic_vectors_100_sentiment = np.array([svd_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "svd_topic_vectors_100_sentiment_doc = pd.DataFrame(svd_topic_vectors_100_sentiment, columns=columns[:100], index=index)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300,n_iter=100)\n",
    "svd_topic_vectors_300 = svd.fit_transform(tfidf_docs)\n",
    "svd_topic_vectors_300 = (svd_topic_vectors_300.T / np.linalg.norm(svd_topic_vectors_300,axis=1)).T\n",
    "svd_topic_vectors_300_doc = pd.DataFrame(svd_topic_vectors_300, columns=columns[:300], index=index)\n",
    "svd_topic_vectors_300_sentiment = np.array([svd_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "svd_topic_vectors_300_sentiment_doc = pd.DataFrame(svd_topic_vectors_300_sentiment, columns=columns[:300], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def lda_predic (name,vector,df,over_sampling=False):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(vector,df.rumor,test_size=0.2 ,random_state=42)\n",
    "    if over_sampling:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        # 进行过采样\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        print(\"过采样前训练集中类别0的数量：\", sum(y_train==0))\n",
    "        print(\"过采样前训练集中类别1的数量：\", sum(y_train==1))\n",
    "\n",
    "        print(\"过采样后训练集中类别0的数量：\", sum(y_train_res==0))\n",
    "        print(\"过采样后训练集中类别1的数量：\", sum(y_train_res==1))\n",
    "\n",
    "    lda = LDA(n_components=1)\n",
    "    lda = lda.fit(X_train_res, y_train_res)\n",
    "    df[name] = lda.predict(vector)\n",
    "    round(float(lda.score(X_test, y_test)), 3)\n",
    "    from sklearn import metrics\n",
    "    y_true = df.loc[y_test.index].rumor\n",
    "    y_pred = df.loc[y_test.index][name]\n",
    "    loss_total = 0\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=[\"non-rumor\",\"rumor\"], digits=4)\n",
    "    confusion = metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(report)\n",
    "    with open(\"report/\"+name+'.txt', 'w', encoding='utf-8') as f:\n",
    "        print(report,file=f)\n",
    "    print(confusion)\n",
    "    with open(\"report_matrix/\"+name+'_matrix'+'.txt', 'w', encoding='utf-8') as f:\n",
    "        print(confusion,file=f)\n",
    "    np.savez(\"report_matrix/\"+name+'_matrix'+'.npz', matrix= confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n"
     ]
    }
   ],
   "source": [
    "lda_predic('tfidf',tfidf_docs,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9564    0.7127    0.8168       369\n",
      "       rumor     0.6241    0.9362    0.7489       188\n",
      "\n",
      "    accuracy                         0.7882       557\n",
      "   macro avg     0.7902    0.8245    0.7829       557\n",
      "weighted avg     0.8442    0.7882    0.7939       557\n",
      "\n",
      "[[263 106]\n",
      " [ 12 176]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_100',pca_topic_vectors_100_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9164    0.7425    0.8204       369\n",
      "       rumor     0.6318    0.8670    0.7309       188\n",
      "\n",
      "    accuracy                         0.7846       557\n",
      "   macro avg     0.7741    0.8048    0.7757       557\n",
      "weighted avg     0.8203    0.7846    0.7902       557\n",
      "\n",
      "[[274  95]\n",
      " [ 25 163]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_300',pca_topic_vectors_300_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8918    0.7371    0.8071       369\n",
      "       rumor     0.6151    0.8245    0.7045       188\n",
      "\n",
      "    accuracy                         0.7666       557\n",
      "   macro avg     0.7534    0.7808    0.7558       557\n",
      "weighted avg     0.7984    0.7666    0.7725       557\n",
      "\n",
      "[[272  97]\n",
      " [ 33 155]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_100_sentiment',pca_topic_vectors_100_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8797    0.7534    0.8117       369\n",
      "       rumor     0.6224    0.7979    0.6993       188\n",
      "\n",
      "    accuracy                         0.7684       557\n",
      "   macro avg     0.7511    0.7756    0.7555       557\n",
      "weighted avg     0.7929    0.7684    0.7737       557\n",
      "\n",
      "[[278  91]\n",
      " [ 38 150]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('pca_300_sentiment',pca_topic_vectors_300_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8780    0.6829    0.7683       369\n",
      "       rumor     0.5667    0.8138    0.6681       188\n",
      "\n",
      "    accuracy                         0.7271       557\n",
      "   macro avg     0.7224    0.7484    0.7182       557\n",
      "weighted avg     0.7730    0.7271    0.7345       557\n",
      "\n",
      "[[252 117]\n",
      " [ 35 153]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_100',svd_topic_vectors_100_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9357    0.7100    0.8074       369\n",
      "       rumor     0.6137    0.9043    0.7312       188\n",
      "\n",
      "    accuracy                         0.7756       557\n",
      "   macro avg     0.7747    0.8071    0.7693       557\n",
      "weighted avg     0.8270    0.7756    0.7817       557\n",
      "\n",
      "[[262 107]\n",
      " [ 18 170]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_300',svd_topic_vectors_300_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8691    0.7019    0.7766       369\n",
      "       rumor     0.5753    0.7926    0.6667       188\n",
      "\n",
      "    accuracy                         0.7325       557\n",
      "   macro avg     0.7222    0.7472    0.7216       557\n",
      "weighted avg     0.7700    0.7325    0.7395       557\n",
      "\n",
      "[[259 110]\n",
      " [ 39 149]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_100_sentiment',svd_topic_vectors_100_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.9347    0.6206    0.7459       369\n",
      "       rumor     0.5513    0.9149    0.6880       188\n",
      "\n",
      "    accuracy                         0.7199       557\n",
      "   macro avg     0.7430    0.7677    0.7170       557\n",
      "weighted avg     0.8053    0.7199    0.7264       557\n",
      "\n",
      "[[229 140]\n",
      " [ 16 172]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('svd_300_sentiment',svd_topic_vectors_300_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7187    0.6369    0.6753       369\n",
      "       rumor     0.4174    0.5106    0.4593       188\n",
      "\n",
      "    accuracy                         0.5943       557\n",
      "   macro avg     0.5680    0.5737    0.5673       557\n",
      "weighted avg     0.6170    0.5943    0.6024       557\n",
      "\n",
      "[[235 134]\n",
      " [ 92  96]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_100',ldia_topic_vectors_100_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7483    0.5962    0.6637       369\n",
      "       rumor     0.4335    0.6064    0.5055       188\n",
      "\n",
      "    accuracy                         0.5996       557\n",
      "   macro avg     0.5909    0.6013    0.5846       557\n",
      "weighted avg     0.6420    0.5996    0.6103       557\n",
      "\n",
      "[[220 149]\n",
      " [ 74 114]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_300',ldia_topic_vectors_300_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7391    0.5528    0.6326       369\n",
      "       rumor     0.4128    0.6170    0.4947       188\n",
      "\n",
      "    accuracy                         0.5745       557\n",
      "   macro avg     0.5760    0.5849    0.5636       557\n",
      "weighted avg     0.6290    0.5745    0.5860       557\n",
      "\n",
      "[[204 165]\n",
      " [ 72 116]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_100_sentiment',ldia_topic_vectors_100_sentiment_doc,df,over_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样前训练集中类别0的数量： 1502\n",
      "过采样前训练集中类别1的数量： 725\n",
      "过采样后训练集中类别0的数量： 1502\n",
      "过采样后训练集中类别1的数量： 1502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.7754    0.5989    0.6758       369\n",
      "       rumor     0.4559    0.6596    0.5391       188\n",
      "\n",
      "    accuracy                         0.6194       557\n",
      "   macro avg     0.6157    0.6292    0.6075       557\n",
      "weighted avg     0.6676    0.6194    0.6297       557\n",
      "\n",
      "[[221 148]\n",
      " [ 64 124]]\n"
     ]
    }
   ],
   "source": [
    "lda_predic('ldia_300_sentiment',ldia_topic_vectors_300_sentiment_doc,df,over_sampling=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d40a5097db4c11f3975da84c5456511726d6f156896da7e31bb0c17b283947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='your module name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/stevenyu/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rumor0!:脑出血不能引起病人剧烈的头痛。\n",
      "不能    1\n",
      "剧烈    1\n",
      "头痛    1\n",
      "引起    1\n",
      "病人    1\n",
      "Name: rumor0!, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv',index_col=0)\n",
    "index = df.index\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=df.seg_text)\n",
    "                        .toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "                                     counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize,)\n",
    "tfidf_docs =tfidf.fit_transform(raw_documents=df.text).toarray()\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('rumor0:'+df.loc['rumor0'].text)\n",
    "    print(bow_docs.loc['rumor0'][bow_docs.loc['rumor0'] > 0].head())\n",
    "except KeyError:\n",
    "    print('rumor0!:'+df.loc['rumor0!'].text)\n",
    "    print(bow_docs.loc['rumor0!'][bow_docs.loc['rumor0!'] > 0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.sentiments\n",
    "sentiment_dict = np.load('sentiment_dict.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia100 = LDiA(n_components=100, learning_method='batch')\n",
    "ldia100 = ldia100.fit(bow_docs)\n",
    "ldia100_topic_vectors = ldia100.transform(bow_docs)\n",
    "columns100 = ['topic{}'.format(i) for i in range(ldia100.components_.shape[0])]\n",
    "ldia100_topic_vectors_doc = pd.DataFrame(ldia100_topic_vectors,index=index, columns=columns100)\n",
    "ldia100_topic_vectors_sentiment = np.array([ldia100_topic_vectors[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia100_topic_vectors_sentiment_doc = pd.DataFrame(ldia100_topic_vectors_sentiment,index=index, columns=columns100)\n",
    "\n",
    "ldia300 = LDiA(n_components=300, learning_method='batch')\n",
    "ldia300 = ldia100.fit(bow_docs)\n",
    "ldia300_topic_vectors = ldia300.transform(bow_docs)\n",
    "columns300 = ['topic{}'.format(i) for i in range(ldia300.components_.shape[0])]\n",
    "ldia300_topic_vectors_doc = pd.DataFrame(ldia300_topic_vectors,index=index, columns=columns300)\n",
    "ldia300_topic_vectors_sentiment = np.array([ldia300_topic_vectors[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia300_topic_vectors_sentiment_doc = pd.DataFrame(ldia300_topic_vectors_sentiment,index=index, columns=columns300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "k=min(len(tfidf.vocabulary_),len(df))\n",
    "k=100\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors_100 = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors_100_doc = pd.DataFrame(pca_topic_vectors_100, columns=columns, index=index)\n",
    "pca_topic_vectors_100_sentiment = np.array([pca_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_topic_vectors_100_sentiment_doc = pd.DataFrame(pca_topic_vectors_100_sentiment, columns=columns, index=index)\n",
    "k=300\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors_300 = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors_300_doc = pd.DataFrame(pca_topic_vectors_300, columns=columns, index=index)\n",
    "pca_topic_vectors_300_sentiment = np.array([pca_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_topic_vectors_300_sentiment_doc = pd.DataFrame(pca_topic_vectors_300_sentiment, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_\n",
    "#根据词项的频率对词汇表进行排序\n",
    "#当对某个不按照最左边元素排序的序列解压并在排序后重新压缩时，可以使用zip(*sorted(zip(...)))\n",
    "column_nums , terms = zip(*sorted(zip(tfidf.vocabulary_.values(), tfidf.vocabulary_.keys())))\n",
    "weights = pd.DataFrame(pca.components_, columns=terms,\n",
    "                       index = ['topic{}'.format(i) for i in range(pca.components_.shape[0])])\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100,n_iter=100)\n",
    "svd_topic_vectors_100 = svd.fit_transform(tfidf_docs)\n",
    "svd_topic_vectors_100 = (svd_topic_vectors_100.T / np.linalg.norm(svd_topic_vectors_100,axis=1)).T\n",
    "svd_topic_vectors_100_doc = pd.DataFrame(svd_topic_vectors_100, columns=columns, index=index)\n",
    "svd_topic_vectors_100_sentiment = np.array([svd_topic_vectors_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "svd_topic_vectors_100_sentiment_doc = pd.DataFrame(svd_topic_vectors_100_sentiment, columns=columns, index=index)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300,n_iter=100)\n",
    "svd_topic_vectors_300 = svd.fit_transform(tfidf_docs)\n",
    "svd_topic_vectors_300 = (svd_topic_vectors_300.T / np.linalg.norm(svd_topic_vectors_300,axis=1)).T\n",
    "svd_topic_vectors_300_doc = pd.DataFrame(svd_topic_vectors_300, columns=columns, index=index)\n",
    "svd_topic_vectors_300_sentiment = np.array([svd_topic_vectors_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "svd_topic_vectors_300_sentiment_doc = pd.DataFrame(svd_topic_vectors_300_sentiment, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def lda_predic (name,vector,df,over_sampling=False):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(vector,df.rumor,test_size=0.2 ,random_state=42)\n",
    "    if over_sampling:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        # 进行过采样\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        print(\"过采样前训练集中类别0的数量：\", sum(y_train==0))\n",
    "        print(\"过采样前训练集中类别1的数量：\", sum(y_train==1))\n",
    "\n",
    "        print(\"过采样后训练集中类别0的数量：\", sum(y_train_res==0))\n",
    "        print(\"过采样后训练集中类别1的数量：\", sum(y_train_res==1))\n",
    "\n",
    "    lda = LDA(n_components=1)\n",
    "    lda = lda.fit(X_train_res, y_train_res)\n",
    "    df['name'] = lda.predict(vector)\n",
    "    round(float(lda.score(X_test, y_test)), 3)\n",
    "    from sklearn import metrics\n",
    "    y_true = df.loc[y_test.index].rumor\n",
    "    y_pred = df.loc[y_test.index].LSA16_rumor\n",
    "    loss_total = 0\n",
    "    acc = metrics.accuracy_score(y_true, y_pred)\n",
    "    report = metrics.classification_report(y_true, y_pred, target_names=[\"non-rumor\",\"rumor\"], digits=4)\n",
    "    confusion = metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(report)\n",
    "    with open(\"../Chinese-Text-Classification-Pytorch/report/\"+name+'.txt', 'w', encoding='utf-8') as f:\n",
    "        print(report,file=f)\n",
    "    print(confusion)\n",
    "    with open(\"../Chinese-Text-Classification-Pytorch/report_matrix/\"+name+'_matrix'+'.txt', 'w', encoding='utf-8') as f:\n",
    "        print(confusion,file=f)\n",
    "    np.savez(\"../Chinese-Text-Classification-Pytorch/report_matrix/\"+name+'_matrix'+'.npz', matrix= confusion)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6d40a5097db4c11f3975da84c5456511726d6f156896da7e31bb0c17b283947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

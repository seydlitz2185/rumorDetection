{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrod_dict = np.load('word_dict.npz',allow_pickle=True)\n",
    "wrod_dict = wrod_dict['word_dict'].item()\n",
    "pca_weights_100 = pd.read_csv('pca_weights_100.csv')\n",
    "pca_weights_300 = pd.read_csv('pca_weights_300.csv')\n",
    "ldia_weights_100 = pd.read_csv('ldia_weights_100.csv')\n",
    "ldia_weights_300 = pd.read_csv('ldia_weights_300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'pygp_data/data/vocab.pkl'\n",
    "vocab_dict = pkl.load(open(vocab_path, 'rb'))\n",
    "vocab_dict_1 = vocab_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = list(vocab_dict.keys())\n",
    "len(vocabs)\n",
    "sentiment_dict = np.load('sentiment_dict.npy',allow_pickle=True).item()\n",
    "sentiments = []\n",
    "for i in range(len(vocabs)):\n",
    "    if vocabs[i] in sentiment_dict.keys():\n",
    "        sentiments.append(sentiment_dict[vocabs[i]])\n",
    "    else:\n",
    "        sentiments.append(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word):\n",
    "    vec = np.zeros(300)\n",
    "    if word in wrod_dict:\n",
    "        vec = wrod_dict[word]\n",
    "        vocab_dict[word] = wrod_dict[word]\n",
    "    return vec\n",
    "\n",
    "def get_pca_vector(word,dim):\n",
    "    vec = np.zeros(dim)\n",
    "    if dim == 100:\n",
    "        if word in pca_weights_100.columns:\n",
    "            vec = pca_weights_100[word].to_numpy()\n",
    "    if dim == 300:\n",
    "        if word in pca_weights_300.columns:\n",
    "            vec = pca_weights_300[word].to_numpy()\n",
    "    return vec\n",
    "\n",
    "def get_ldia_vector(word,dim):\n",
    "    vec = np.zeros(dim)\n",
    "    if dim == 100:\n",
    "        if word in ldia_weights_100.columns:\n",
    "            vec = ldia_weights_100[word].to_numpy()\n",
    "    if dim == 300:\n",
    "        if word in ldia_weights_300.columns:\n",
    "            vec = ldia_weights_300[word].to_numpy()\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= []\n",
    "pca_embeddings_100 = []\n",
    "pca_embeddings_300 = []\n",
    "ldia_embeddings_100 = []\n",
    "ldia_embeddings_300 = []\n",
    "for vocab in vocabs:\n",
    "    vec = get_word_vector(vocab)\n",
    "    pca_vec_100 = get_pca_vector(vocab,100)\n",
    "    pca_vec_300 = get_pca_vector(vocab,300)\n",
    "    ldia_vec_100 = get_ldia_vector(vocab,100)\n",
    "    ldia_vec_300 = get_ldia_vector(vocab,300)\n",
    "    vocab_dict[vocab] = vec\n",
    "    embeddings.append(vec)\n",
    "    pca_embeddings_100.append(pca_vec_100)\n",
    "    pca_embeddings_300.append(pca_vec_300)\n",
    "    ldia_embeddings_100.append(ldia_vec_100)\n",
    "    ldia_embeddings_300.append(ldia_vec_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('pygp_data/data/embedding_cc.zh.300.npz',embeddings = embeddings)\n",
    "np.savez('pygp_data/data/embedding_pca.zh.100.npz',embeddings = pca_embeddings_100)\n",
    "np.savez('pygp_data/data/embedding_pca.zh.300.npz',embeddings = pca_embeddings_300)\n",
    "np.savez('pygp_data/data/embedding_ldia.zh.100.npz',embeddings = ldia_embeddings_100)\n",
    "np.savez('pygp_data/data/embedding_ldia.zh.300.npz',embeddings = ldia_embeddings_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_embeddings_300_sentiment =np.array([embeddings[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_embeddings_300_sentiment =np.array([pca_embeddings_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "pca_embeddings_100_sentiment =np.array([pca_embeddings_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia_embeddings_100_sentiment =np.array([ldia_embeddings_100[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "ldia_embeddings_300_sentiment =np.array([ldia_embeddings_300[i]*sentiments[i] for i in range(len(sentiments))])\n",
    "np.savez('pygp_data/data/embedding_cc.zh.300_sentiment.npz',embeddings = fastText_embeddings_300_sentiment)\n",
    "np.savez('pygp_data/data/embedding_pca.zh.100_sentiment.npz',embeddings = pca_embeddings_100_sentiment)\n",
    "np.savez('pygp_data/data/embedding_pca.zh.300_sentiment.npz',embeddings = pca_embeddings_300_sentiment)\n",
    "np.savez('pygp_data/data/embedding_ldia.zh.100_sentiment.npz',embeddings = ldia_embeddings_100_sentiment)\n",
    "np.savez('pygp_data/data/embedding_ldia.zh.300_sentiment.npz',embeddings = ldia_embeddings_300_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
